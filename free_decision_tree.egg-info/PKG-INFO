Metadata-Version: 2.1
Name: free_decision_tree
Version: 0.0.4
Summary: Modifiable decision tree.
Author: ['Ian dos Anjos Melo Aguiar']
Author-email: <iannaianjos@gmail.com>
Keywords: python,regression
Classifier: Development Status :: 1 - Planning
Classifier: Intended Audience :: Developers
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: Unix
Classifier: Operating System :: MacOS :: MacOS X
Classifier: Operating System :: Microsoft :: Windows
Description-Content-Type: text/markdown
Requires-Dist: pandas
Requires-Dist: matplotlib
Requires-Dist: numpy


# DecisionTree

A customizable implementation of decision trees for regression, written from scratch with flexible loss functions and visualization tools.

---

## Download

```bash
pip install git+https://github.com/IanAguiar-ai/free_decision_tree
```

---

## Inputs

When creating a `DecisionTree`, you can specify:

- **data** (`pd.DataFrame`): Input dataset (features + target column).
- **y** (`str`): Name of the target column.
- **max_depth** (`int`, default=`3`): Maximum depth of the tree.
- **min_samples** (`int`, default=`1`): Minimum samples required in a node.
- **loss_function** (`callable`, default=`simple_loss`): Function used to compute individual losses.
- **loss_calc** (`callable`, default=`calc_loss`): Function used to combine losses from splits.
- **plot** (`Plot`, optional): Progress bar handler; if `None`, a default progress loader is used.
- **train** (`bool`, default=`True`): If `True`, trains the tree at initialization.
- **depth** (`int`, internal): Current depth (automatically managed during recursion).
- **print** (`bool`, default=`False`): Enables verbose logging during training and prediction.
- **optimized** (`int`, default=`-1`): Step size for tested split points (reduces computation).

---

## Use

### Example

```python
import pandas as pd
from free_decision_tree import DecisionTree

# Example dataset
df = pd.DataFrame({
    "x1": [1, 2, 3, 4, 5],
    "x2": [2, 1, 4, 3, 5],
    "y":  [1.2, 1.9, 3.1, 3.9, 5.1]
})

# Train tree
tree = DecisionTree(df, y="y", max_depth=2, min_samples=1)

# Print structure
print(tree)
```

---

### Prediction

```python
# Predict on new data
X_test = pd.DataFrame({"x1": [2.5], "x2": [3.0]})
prediction = tree.predict(X_test)

print(prediction)  # Returns float or list of floats
```

You can also call the tree directly:

```python
prediction = tree(X_test)
```

---

### Predict smooth

```python
tree.predict_smooth(X_test, n_neighbors=None, alpha=0.001)
```

Performs prediction with a **smoothing technique based on nearest neighbors** of the leaves.
Instead of returning only the raw leaf output, it:

1. Calls `detect_depth()` (if not already computed) to build a table of leaves and their average positions/outputs.
2. For each row in `X_test`, computes the Euclidean distance to each averaged leaf centroid.
3. Selects the *n_neighbors* closest leaves (default = number of features + 1).
4. Applies distance-weighted averaging using parameter `alpha` to avoid division by zero.

Parameters:
- **`X_test`** (`pd.DataFrame`): Input data for prediction.
- **`n_neighbors`** (`int`, optional): Number of neighbors to consider; defaults to `len(features) + 1`.
- **`alpha`** (`float`, default=`0.001`): Smoothing factor to stabilize weights.

Returns:
- A single float if one row is passed, or a list of floats otherwise.

This method is recommended when predictions should be less sensitive to sharp splits of the tree, approximating a smoother regression surface.

---

### Detect depth

```python
tree.detect_depth()
```

Generates a new `DataFrame` containing metadata about each sample in the training set:
- **`__dt_depth__`**: the depth of the node in which the sample ended up.
- **`__dt_leaf__`**: the identifier of the leaf reached by the sample.
- **`__dt_y__`**: the predicted output value for the sample.

This is useful to analyze how samples are distributed across the tree, debug overfitting, or prepare structures for smoothing techniques.

Returns:
- A copy of the training `DataFrame` with the additional diagnostic columns.

---

### Plot tree

```python
tree.plot_tree()
```

Generates a visual representation of the decision tree, with splits, sample counts, losses, and outputs at each node.

---

### Plot sensitivity

```python
tree.plot_sensitivity(train=df, test=df, y="y")
```

Performs a sensitivity test over different depths.
Outputs:
- MSE (train vs. test) for each depth.
- A plot with the optimal depth (minimum test error).
- Returns the best depth as `int`.

---

### Plot confidence interval

```python
tree.plot_ci(test=df, y="y", confidence=0.95)
```

Generates a scatter plot of predicted vs. real values with confidence intervals.
Returns the approximate confidence error value (`float`).

---

## Modifiable parts

- **Individual losses** (`loss_function`)
  By default, variance loss (`simple_loss`). Can be replaced with custom metrics (e.g., entropy, Gini).

```python
def simple_loss(y:pd.DataFrame) -> float:
    y_:float = y.mean()
    return sum([(y_i - y_)*(y_i - y_) for y_i in y])
```

- **Merging of losses** (`loss_calc`)
  By default, additive (`calc_loss`). Can be changed to `max`, weighted average, etc.

```python
def calc_loss(loss_1:float, loss_2:float) -> float:
    return loss_1 + loss_2 #max(loss_1, loss_2)
```

- **Function to prediction in leaf** (`function_prediction_leaf`)
  By default, additive (`mean`). Can be changed.
 
```python
def mean(dt:pd.DataFrame):
    return dt.mean()
```

---

## Structure

- **`DecisionTree`**: Main class (training, prediction, visualization).
- **`Plot`**: Progress bar utility.
- **`simple_loss(y)`**: Default variance-based loss.
- **`calc_loss(loss_1, loss_2)`**: Default method to merge losses.
